{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a174aae",
   "metadata": {},
   "source": [
    "**Installation:**\n",
    "\n",
    "1. Run multiple Cassandra nodes on separate Docker containers locally and connect them in a cluster to simulate a distributed environment.\n",
    "\n",
    "2. Configure the nodes to communicate with each other and form a cluster. This typically involves configuring the seeds nodes (initial nodes in the cluster), setting the same cluster name for all nodes, and configuring the network to allow communication between the nodes.\n",
    "\n",
    "3. Once the nodes are configured and running, use Cassandra's nodetool utility to verify that the nodes are part of the same cluster and are communicating with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8230f6",
   "metadata": {},
   "source": [
    "**Scenario:**\n",
    "\n",
    "Consider a 4 node Cassandra cluster. \n",
    "\n",
    "We have a table called \"customer_orders\" with a partition key of \"customer_id\". The replication factor for this table is set to 2, which means that each partition in the table will have 2 replicas, for a total of 3 copies of each partition in the cluster.\n",
    "\n",
    "Consider the case where the customer_id is 12345. The hash function applied to this customer_id would result in a token, let's say, 500. Now, the token ranges for the 4 nodes in the cluster can be divided as follows:\n",
    "\n",
    "- Node 1: token range 0 to 250\n",
    "- Node 2: token range 251 to 500\n",
    "- Node 3: token range 501 to 750\n",
    "- Node 4: token range 751 to 1000\n",
    "\n",
    "Since the token for customer_id 12345 is 500, it would fall in the token range of Node 2. Therefore, Node 2 would store the primary partition for customer_id 12345.\n",
    "\n",
    "Since the replication factor for this table is 2, the two replicas of the partition would also be stored in the cluster. To ensure that the replicas are stored on different nodes, Cassandra uses the concept of snitch to determine the optimal placement of the replicas. The snitch might place one replica on Node 1 (since it's closest to Node 2) and another replica on Node 3 (since it's furthest from Node 2).\n",
    "\n",
    "Node 1:\n",
    "- Token range: 0 to 250\n",
    "- Partitions stored: One of the replicas for customer_id 12345 (token 500) and any partition whose token falls within the range of 0 to 250\n",
    "\n",
    "Node 2:\n",
    "- Token range: 251 to 500\n",
    "- Partitions stored: The primary partition for customer_id 12345 (token 500) and any other partition whose token falls within the range of 251 to 500\n",
    "\n",
    "Node 3:\n",
    "- Token range: 501 to 750\n",
    "- Partitions stored: One of the replicas for customer_id 12345 (token 500) and any other partition whose token falls within the range of 501 to 750\n",
    "\n",
    "<br>\n",
    "\n",
    "This way, even if one node fails, the data remains available as the replicas are stored on different nodes in the cluster. Additionally, if a query is performed on the customer_id 12345, Cassandra can quickly retrieve the data from the closest replica, ensuring low latency and high performance.\n",
    "\n",
    "So, in this example, the primary partition for customer_id 12345 would be stored on Node 2, and its two replicas would be stored on Node 1 and Node 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5938cba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
